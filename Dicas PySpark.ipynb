{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicas PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quando você tem consultas frequentes em grandes tabelas que aplicam filtros (WHERE, JOIN, etc.) em colunas específicas, como date, customer_id, ou region.\n",
    "OPTIMIZE my_table\n",
    "ZORDER BY (customer_id, date);\n",
    "\n",
    "#Isso otimiza a tabela para consultas baseadas em customer_id e date.\n",
    "#Benefícios:\n",
    "#Reduz o IO (Input/Output) ao acessar apenas os arquivos relevantes.\n",
    "#Melhora o desempenho de consultas em tabelas muito grandes com muitos arquivos particionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando as propriedades do Delta Lake TIME TRAVEL: \"VERSION AS OF\", \"TIMESTAMP AS OF\", \"DESCRIBE HISTORY\", \"RESTORE TABLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cenário:\n",
    "#Você gerencia uma tabela de pacientes chamada pacientes_delta que contém dados sensíveis sobre histórico médico. \n",
    "# Por engano, um analista deletou ou alterou registros importantes, e agora você precisa restaurar os dados para o estado em que estavam antes do incidente.\n",
    "\n",
    "CREATE TABLE pacientes_delta (\n",
    "    id INT,\n",
    "    nome STRING,\n",
    "    diagnostico STRING\n",
    ") USING DELTA;\n",
    "\n",
    "INSERT INTO pacientes_delta VALUES\n",
    "    (1, 'Maria', 'Hipertensão'),\n",
    "    (2, 'João', 'Diabetes'),\n",
    "    (3, 'Ana', 'Asma');\n",
    "---\n",
    "UPDATE pacientes_delta\n",
    "SET diagnostico = 'Nenhum'\n",
    "WHERE id = 2;\n",
    "---\n",
    "SELECT * FROM pacientes_delta VERSION AS OF 0;\n",
    "SELECT * FROM pacientes_delta TIMESTAMP AS OF '2025-01-15T10:00:00.000+0000';\n",
    "RESTORE TABLE pacientes_delta TO VERSION AS OF 0;\n",
    "--\n",
    "#Você pode usar o comando para identificar as versões da tabela, para então identificar qual estada deseja fazer o restore. \n",
    "DESCRIBE HISTORY nome_da_tabela;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular o IMC (Índice de Massa Corporal) de pacientes a partir de suas alturas e pesos. O cálculo não está disponível nativamente no Spark.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Função Python para calcular o IMC\n",
    "def calcular_imc(peso, altura):\n",
    "    if altura > 0:  # Evita divisão por zero\n",
    "        return peso / (altura ** 2)\n",
    "    return None\n",
    "\n",
    "# Registrar a função como UDF\n",
    "calcular_imc_udf = udf(calcular_imc, DoubleType())\n",
    "\n",
    "# Criar DataFrame com dados de exemplo\n",
    "dados = [(1, 70.0, 1.75), (2, 80.0, 1.80), (3, 90.0, 0)]  # ID, Peso, Altura\n",
    "colunas = [\"id\", \"peso\", \"altura\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Aplicar a UDF no DataFrame\n",
    "df_com_imc = df.withColumn(\"imc\", calcular_imc_udf(df.peso, df.altura))\n",
    "\n",
    "# Mostrar o resultado\n",
    "df_com_imc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escrever uma UDF para converter strings em letras maiúsculas, simulando uma necessidade customizada.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Função Python para converter texto para maiúsculas\n",
    "def para_maiusculas(texto):\n",
    "    if texto:\n",
    "        return texto.upper()\n",
    "    return None\n",
    "\n",
    "# Registrar como UDF\n",
    "para_maiusculas_udf = udf(para_maiusculas, StringType())\n",
    "\n",
    "# Criar DataFrame com dados de exemplo\n",
    "dados = [(\"João\",), (\"Maria\",), (\"Carlos\",)]\n",
    "colunas = [\"nome\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Aplicar a UDF\n",
    "df_maiusculo = df.withColumn(\"nome_maiusculo\", para_maiusculas_udf(df.nome))\n",
    "\n",
    "# Mostrar o resultado\n",
    "df_maiusculo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar uma UDF que classifique a idade em faixas etárias com base em múltiplas condições.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Função para categorizar faixa etária\n",
    "def faixa_etaria(idade):\n",
    "    if idade < 18:\n",
    "        return \"Menor de idade\"\n",
    "    elif 18 <= idade < 60:\n",
    "        return \"Adulto\"\n",
    "    else:\n",
    "        return \"Idoso\"\n",
    "\n",
    "# Registrar como UDF\n",
    "faixa_etaria_udf = udf(faixa_etaria, StringType())\n",
    "\n",
    "# Criar DataFrame com dados de exemplo\n",
    "dados = [(15,), (25,), (65,)]\n",
    "colunas = [\"idade\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Aplicar a UDF\n",
    "df_faixa = df.withColumn(\"faixa_etaria\", faixa_etaria_udf(df.idade))\n",
    "\n",
    "# Mostrar o resultado\n",
    "df_faixa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converter strings para letras maiúsculas pode ser feito com uma função nativa do Spark:\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# Criar DataFrame com dados de exemplo\n",
    "dados = [(\"João\",), (\"Maria\",), (\"Carlos\",)]\n",
    "colunas = [\"nome\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Usar função nativa em vez de UDF\n",
    "df_maiusculo = df.withColumn(\"nome_maiusculo\", upper(df.nome))\n",
    "\n",
    "# Mostrar o resultado\n",
    "df_maiusculo.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidando com dados PII (Personally Identifiable Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar máscara em colunas sensíveis como CPF, números de cartão de crédito ou nomes.\n",
    "from pyspark.sql.functions import lit, concat, substring\n",
    "\n",
    "# Criar DataFrame com PII\n",
    "dados = [(\"João Silva\", \"123.456.789-00\", \"5555-4444-3333-2222\"),\n",
    "         (\"Maria Santos\", \"987.654.321-11\", \"6666-5555-4444-3333\")]\n",
    "colunas = [\"nome\", \"cpf\", \"cartao_credito\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Máscara de dados\n",
    "df_masked = df.withColumn(\"cpf\", concat(lit(\"***.***.\"), substring(df[\"cpf\"], 9, 3), lit(\"-**\"))) \\\n",
    "              .withColumn(\"cartao_credito\", concat(lit(\"****-****-****-\"), substring(df[\"cartao_credito\"], 16, 4)))\n",
    "\n",
    "df_masked.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substituir números de CPF por tokens:\n",
    "from pyspark.sql.functions import sha2, concat\n",
    "\n",
    "# Tokenizar CPF com hash SHA-256\n",
    "df_tokenized = df.withColumn(\"cpf_tokenizado\", sha2(concat(df[\"cpf\"]), 256))\n",
    "\n",
    "df_tokenized.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substituir nomes reais por nomes fictícios:\n",
    "from faker import Faker\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Função para gerar nomes fictícios\n",
    "def pseudonimizar_nome(nome):\n",
    "    return fake.name()\n",
    "\n",
    "# Registrar UDF\n",
    "pseudonimizar_udf = udf(pseudonimizar_nome, StringType())\n",
    "\n",
    "# Aplicar pseudonimização\n",
    "df_pseudo = df.withColumn(\"nome_pseudonimizado\", pseudonimizar_udf(df.nome))\n",
    "\n",
    "df_pseudo.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo das Boas Práticas para Dados PII no Databricks\n",
    "| **Estratégia**          | **Benefício**                                                                 |\n",
    "|--------------------------|------------------------------------------------------------------------------|\n",
    "| **Criptografia**         | Protege dados em trânsito e em repouso.                                      |\n",
    "| **Máscara de Dados**     | Evita exposição direta de informações sensíveis.                             |\n",
    "| **Controle de Acesso (RBAC)** | Restringe permissões com base em funções e grupos.                         |\n",
    "| **Tokenização**          | Substitui valores reais por tokens não reversíveis.                          |\n",
    "| **Pseudonimização**      | Preserva o formato dos dados, mas substitui valores reais.                   |\n",
    "| **Monitoramento**        | Rastreia acessos e alterações nos dados para auditoria e compliance.         |\n",
    "| **Delta Sharing**        | Permite compartilhamento seguro com parceiros externos.                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificação de qualidade dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: Contagem Total de Linhas\n",
    "# Objetivo: Validar se o número de linhas processadas corresponde ao valor esperado.\n",
    "# Uso: Detectar perda ou duplicação de dados durante ingestões, transformações ou cargas.\n",
    "# DataFrame de exemplo\n",
    "dados = [(1, \"João\", 25), (2, \"Maria\", 30), (3, \"Carlos\", 40)]\n",
    "colunas = [\"id\", \"nome\", \"idade\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Contagem total de linhas\n",
    "contagem_atual = df.count()\n",
    "\n",
    "# Valor esperado\n",
    "valor_esperado = 3\n",
    "\n",
    "# Validação\n",
    "if contagem_atual == valor_esperado:\n",
    "    print(f\"✅ Contagem de linhas correta: {contagem_atual}\")\n",
    "else:\n",
    "    print(f\"⚠️ Contagem de linhas incorreta! Atual: {contagem_atual}, Esperado: {valor_esperado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: Contagem de NULL\n",
    "# Objetivo: Identificar problemas de JOIN, ETL ou dados ausentes em colunas críticas.\n",
    "# Uso: Detectar erros em processos de união de tabelas ou ingestões incompletas.\n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# DataFrame de exemplo com valores NULL\n",
    "dados = [(1, \"João\", 25), (2, None, 30), (3, \"Carlos\", None)]\n",
    "colunas = [\"id\", \"nome\", \"idade\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Contagem de valores NULL por coluna\n",
    "df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\")\n",
    "    for c in df.columns\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: Contagem de Chaves Exclusivas\n",
    "# Objetivo: Garantir que não há duplicidade em chaves primárias.\n",
    "# Uso: Evitar erros em tabelas relacionais e garantir integridade referencial.\n",
    "# Se tiver retorno, tem algo errado na chave PK\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# DataFrame de exemplo com duplicados\n",
    "dados = [(1, \"João\", 25), (2, \"Maria\", 30), (2, \"Maria\", 30), (3, \"Carlos\", 40)]\n",
    "colunas = [\"id\", \"nome\", \"idade\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Contar duplicados na chave primária (id)\n",
    "df.groupBy(\"id\").count().filter(col(\"count\") > 1).show()\n",
    "\n",
    "# Se o DataFrame retornar resultados, significa que há duplicados:\n",
    "duplicados = df.groupBy(\"id\").count().filter(col(\"count\") > 1).count()\n",
    "if duplicados > 0:\n",
    "    print(f\"⚠️ Foram encontrados {duplicados} IDs duplicados!\")\n",
    "else:\n",
    "    print(\"✅ Nenhum ID duplicado encontrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando Checks Automatizados ao Pipeline\n",
    "def verificar_qualidade(df, valor_esperado):\n",
    "    # 1. Check: Contagem Total de Linhas\n",
    "    contagem_atual = df.count()\n",
    "    if contagem_atual != valor_esperado:\n",
    "        print(f\"⚠️ Contagem de linhas incorreta! Atual: {contagem_atual}, Esperado: {valor_esperado}\")\n",
    "\n",
    "    # 2. Check: Contagem de NULL\n",
    "    null_counts = df.select([\n",
    "        count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\")\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    null_counts.show()\n",
    "\n",
    "    # 3. Check: Chaves Exclusivas\n",
    "    duplicados = df.groupBy(\"id\").count().filter(col(\"count\") > 1).count()\n",
    "    if duplicados > 0:\n",
    "        print(f\"⚠️ Foram encontrados {duplicados} IDs duplicados!\")\n",
    "    else:\n",
    "        print(\"✅ Nenhum ID duplicado encontrado.\")\n",
    "\n",
    "# Exemplo de aplicação\n",
    "verificar_qualidade(df, valor_esperado=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como Minimizar o Impacto dos Shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como Identificar Shuffles no Databricks\n",
    "# Spark UI\n",
    "# \n",
    "# Acesse a aba Stages e procure por tarefas que envolvam Shuffle Read e Shuffle Write.\n",
    "# Verifique o tamanho dos dados movidos e o tempo gasto.\n",
    "# Execution Plan\n",
    "# \n",
    "# Analise o plano de execução lógico ou físico para identificar operadores que causam shuffle.\n",
    "\n",
    "df.explain(True)  # Exibe o plano de execução detalhado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolha colunas adequadas para particionar os dados antes de executar operações.\n",
    "df = df.repartition(\"id\")  # Particiona os dados por 'id'\n",
    "\n",
    "#Use coalesce para reduzir partições ao invés de repartition, quando possível, já que coalesce evita shuffle.\n",
    "df = df.coalesce(5)  # Reduz as partições para 5\n",
    "\n",
    "#Salve os dados em tabelas Delta com particionamento antes de realizar operações custosas:\n",
    "df.write.format(\"delta\").partitionBy(\"categoria\").save(\"/path/delta_table\")\n",
    "\n",
    "#Use Z-Ordering no Delta Lake para otimizar leituras e minimizar a movimentação de dados:\n",
    "OPTIMIZE delta_table\n",
    "ZORDER BY (coluna_chave);\n",
    "\n",
    "#Se as partições estiverem desbalanceadas (skew), trate as chaves problemáticas separadamente:\n",
    "skewed_keys = df.filter(df[\"chave\"] == \"chave_problemática\")\n",
    "balanced_df = df.filter(df[\"chave\"] != \"chave_problemática\")\n",
    "result = balanced_df.union(skewed_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Por que os Spills Ocorrem?**\n",
    "Os spills geralmente acontecem devido a:\n",
    "1. **Falta de Memória Suficiente:**\n",
    "   - A memória disponível para uma tarefa não é suficiente para armazenar os dados necessários.\n",
    "   - Por exemplo, ao realizar operações como **sort**, **join** ou **aggregation**, que exigem reorganização ou combinação de grandes volumes de dados.\n",
    "\n",
    "2. **Dados Muito Grandes em Partições:**\n",
    "   - Quando uma partição contém um volume de dados desproporcionalmente grande, ela pode exceder a capacidade de memória disponível.\n",
    "\n",
    "3. **Configuração Inadequada:**\n",
    "   - Parâmetros de configuração do Spark relacionados à memória e ao tamanho das partições não estão ajustados adequadamente.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tipos de Spill**\n",
    "1. **Shuffle Spill:**\n",
    "   - Ocorre durante operações de shuffle (ex.: `JOIN`, `GROUP BY`, `ORDER BY`) quando os dados intermediários gerados pelas tarefas não cabem na memória e precisam ser gravados no disco.\n",
    "\n",
    "2. **Sort Spill:**\n",
    "   - Ocorre durante operações de ordenação (`ORDER BY` ou `SORT`), quando o Spark tenta ordenar dados que não cabem em memória.\n",
    "\n",
    "3. **Aggregation Spill:**\n",
    "   - Acontece em agregações (`SUM`, `COUNT`, etc.), onde o Spark precisa manter estados intermediários em memória para computar os resultados, mas os estados são muito grandes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Impactos dos Spills**\n",
    "Os spills podem causar:\n",
    "1. **Aumento da Latência:**\n",
    "   - A gravação e leitura de dados do disco é muito mais lenta do que manter os dados em memória.\n",
    "   \n",
    "2. **Sobrecarga no Disco:**\n",
    "   - Um uso excessivo de spill pode sobrecarregar o sistema de disco do cluster, causando gargalos em tarefas paralelas.\n",
    "\n",
    "3. **Redução do Desempenho Geral:**\n",
    "   - Quando spills ocorrem com frequência, o tempo de execução dos jobs aumenta significativamente.\n",
    "\n",
    "---\n",
    "\n",
    "### **Como Identificar Spills no Databricks**\n",
    "1. **Spark UI:**\n",
    "   - Acesse a aba **SQL** ou **Stages** no Spark UI para identificar tarefas com:\n",
    "     - **Shuffle Spill** (dados gravados no disco durante o shuffle).\n",
    "     - **Sort Spill** (dados gravados no disco durante a ordenação).\n",
    "   - As colunas relevantes incluem:\n",
    "     - **Shuffle spill (disk):** Quantidade de dados escritos no disco.\n",
    "     - **Memory spill:** Quantidade de dados transferidos para o disco.\n",
    "\n",
    "2. **Execution Plan:**\n",
    "   - Use `explain()` para analisar o plano de execução de uma query e identificar operações que possam causar spill (como shuffles ou sorts):\n",
    "   ```python\n",
    "   df.groupBy(\"coluna\").count().explain(True)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Como Minimizar Spills**\n",
    "1. **Aumente a Memória do Executor:**\n",
    "   - Ajuste a memória disponível para os executores no Spark:\n",
    "     ```bash\n",
    "     spark.executor.memory=8g\n",
    "     spark.executor.memoryOverhead=2g\n",
    "     ```\n",
    "\n",
    "2. **Reduza o Tamanho das Partições:**\n",
    "   - Use particionamento inteligente para evitar partições muito grandes que possam causar spills:\n",
    "     ```python\n",
    "     df = df.repartition(10)  # Divida os dados em 10 partições\n",
    "     ```\n",
    "\n",
    "3. **Ajuste Parâmetros de Configuração:**\n",
    "   - **spark.sql.shuffle.partitions:** Ajuste o número de partições para operações de shuffle.\n",
    "     ```python\n",
    "     spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "     ```\n",
    "   - **spark.sql.autoBroadcastJoinThreshold:** Configure o tamanho limite para joins em broadcast (evitando shuffles grandes).\n",
    "     ```python\n",
    "     spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "     ```\n",
    "\n",
    "4. **Use Tabelas Delta Otimizadas:**\n",
    "   - Compacte e otimize os dados para evitar pequenos arquivos que aumentam o risco de spills:\n",
    "     ```sql\n",
    "     OPTIMIZE tabela_delta\n",
    "     ZORDER BY (coluna_chave);\n",
    "     ```\n",
    "\n",
    "5. **Monitoramento Regular:**\n",
    "   - Acompanhe regularmente os jobs no **Spark UI** e ajuste configurações conforme necessário para evitar spills recorrentes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Exemplo de Código com Ajustes**\n",
    "```python\n",
    "# Configurações para minimizar spills\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # Ajuste o número de partições\n",
    "spark.conf.set(\"spark.executor.memory\", \"8g\")  # Aumente a memória do executor\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"20MB\")  # Limite para joins em broadcast\n",
    "\n",
    "# DataFrame de exemplo\n",
    "dados = [(1, \"João\", 1000), (2, \"Maria\", 1500), (3, \"Carlos\", 2000)]\n",
    "colunas = [\"id\", \"nome\", \"salario\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "# Reparticionar para evitar grandes partições\n",
    "df = df.repartition(\"id\")\n",
    "\n",
    "# Operação que pode causar spill (Join)\n",
    "outro_df = spark.createDataFrame([(1, \"SP\"), (2, \"RJ\"), (3, \"MG\")], [\"id\", \"estado\"])\n",
    "df_join = df.join(outro_df, \"id\")\n",
    "\n",
    "# Exibir o resultado\n",
    "df_join.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- **Spill** é a gravação temporária de dados no disco quando a memória é insuficiente.\n",
    "- Pode ocorrer em operações como **joins**, **aggregations**, ou **sorts**.\n",
    "- **Impacto:** Aumenta o tempo de execução e sobrecarrega os recursos de disco.\n",
    "- **Como evitar:** Ajuste a configuração de memória, particionamento e otimize os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
