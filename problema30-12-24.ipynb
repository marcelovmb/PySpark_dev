{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentação: Resolução de Problema no Azure Storage + Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "Durante a execução de scripts automatizados no Databricks, um erro foi identificado ao tentar gravar dados no catálogo. Apesar do caminho existir no Azure Storage, o script falhava ao encontrar a pasta devido a um espaçamento inicial no nome do diretório, como \" abcd\".\n",
    "\n",
    "Esse problema gerava inconsistências no pipeline, impossibilitando a gravação e o acesso correto aos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução Implementada\n",
    "A solução seguiu os passos abaixo para garantir a resolução e evitar problemas futuros:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listar Pastas no Diretório do Azure Storage\n",
    "O primeiro passo foi listar todas as pastas no diretório do Azure Storage para verificar o problema e identificar os caminhos exatos com o comando dbutils.fs.ls():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"Link/caminho abfss do storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Excluir Tabelas do Catálogo no Databricks\n",
    "Antes de apagar as pastas no storage, foi necessário excluir as tabelas associadas no catálogo do Databricks. As tabelas foram especificadas em uma lista e removidas utilizando o comando DROP TABLE IF EXISTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelas_para_excluir = [\n",
    "    \"catalogo.squema.tabelaA1\",\n",
    "    \"catalogo.squema.tabelaB2\",\n",
    "    \"catalogo.squema.tabelaC3\"\n",
    "]\n",
    "\n",
    "# Exclui tabelas\n",
    "for tabela in tabelas_para_excluir:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {tabela}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Verificar a Exclusão das Tabelas\n",
    "Para garantir que as tabelas foram efetivamente excluídas, foi realizado um loop que verificava a existência de cada tabela no catálogo e registrava os resultados em um DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelas_excluidas = []\n",
    "for tabela in tabelas_para_excluir:\n",
    "    db, tbl = tabela.rsplit('.', 1)\n",
    "    tables = spark.sql(f\"SHOW TABLES IN {db}\")\n",
    "    table_exits = tables.filter(tables.tableName == tbl).count() > 0\n",
    "    tabelas_excluidas.append({\"Table\": tabela, \"Dropped\": not table_exits})\n",
    "\n",
    "# Cria um dataframe com o resultado\n",
    "df_result = spark.createDataFrame(tabelas_excluidas)\n",
    "display(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Excluir Pastas do Azure Storage\n",
    "Após a exclusão das tabelas, as pastas problemáticas no Azure Storage foram removidas usando o comando dbutils.fs.rm():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminhos_para_excluir = [\n",
    "    \"caminho_para_ser_excluidoA1\",\n",
    "    \"caminho_para_ser_excluidoB2\",\n",
    "    \"caminho_para_ser_excluidoC3\"\n",
    "]\n",
    "\n",
    "# Exclui pastas\n",
    "for caminho in caminhos_para_excluir:\n",
    "    dbutils.fs.rm(caminho, recurse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações e Melhorias Futuras\n",
    "1-Validação de Nomes de Pastas: Implementar um script para validar nomes de diretórios automaticamente, removendo espaços ou caracteres inválidos antes da execução do pipeline. \\n\n",
    "\n",
    "2-Logs Detalhados: Adicionar logs em cada etapa do processo para rastreabilidade e depuração, registrando ações como exclusão de tabelas e remoção de pastas. \\n\n",
    "\n",
    "3-Automação do Pipeline: Criar uma função que combine todas as etapas descritas em um pipeline robusto e modular. \\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "Este processo garantiu a resolução do problema e evitou que o espaçamento nos nomes de pastas causasse erros no pipeline. A abordagem foi replicável e documentada para uso em situações semelhantes, contribuindo para maior eficiência e confiabilidade no ambiente de dados."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
